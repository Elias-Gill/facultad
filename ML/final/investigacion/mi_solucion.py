{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":111543,"databundleVersionId":14348714,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# # Estrategia de Predicción del S&P 500\n# ## ElasticNet + LightGBM + GARCH + Stacking Ridge\n# ### Walk-Forward + Target Volatility + Full Pipeline\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-12-04T16:20:28.473904Z\",\"iopub.execute_input\":\"2025-12-04T16:20:28.474715Z\",\"iopub.status.idle\":\"2025-12-04T16:20:28.480346Z\",\"shell.execute_reply.started\":\"2025-12-04T16:20:28.474679Z\",\"shell.execute_reply\":\"2025-12-04T16:20:28.479244Z\"}}\nimport numpy as np, pandas as pd, polars as pl\nfrom sklearn.linear_model import ElasticNetCV, RidgeCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\nimport lightgbm as lgb\nfrom arch import arch_model\nimport warnings, joblib, os\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## 1. Carga y limpieza\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-12-04T16:20:28.489780Z\",\"iopub.execute_input\":\"2025-12-04T16:20:28.490121Z\",\"iopub.status.idle\":\"2025-12-04T16:20:28.583218Z\",\"shell.execute_reply.started\":\"2025-12-04T16:20:28.490095Z\",\"shell.execute_reply\":\"2025-12-04T16:20:28.582346Z\"}}\ntrain = pl.read_csv(\"/kaggle/input/hull-tactical-market-prediction/train.csv\")\ntrain = train.rename({\"market_forward_excess_returns\": \"target\"})\ntrain = train.with_columns(pl.all().cast(pl.Float64, strict=False))\ntrain = train.drop_nulls(subset=[\"target\"])\n\nfeatures = [c for c in train.columns if c not in [\"date_id\", \"target\", \"forward_returns\", \"risk_free_rate\"]]\nprint(f\"Features iniciales: {len(features)}\")\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## 2. Walk-Forward Validation + OOF Stacking\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-12-04T16:20:28.584555Z\",\"iopub.execute_input\":\"2025-12-04T16:20:28.584959Z\",\"iopub.status.idle\":\"2025-12-04T16:20:40.584970Z\",\"shell.execute_reply.started\":\"2025-12-04T16:20:28.584920Z\",\"shell.execute_reply\":\"2025-12-04T16:20:40.583910Z\"}}\ndf = train.to_pandas().set_index(\"date_id\")\n\nfinal_features = [c for c in df.columns if c != \"target\"]\nX = df[final_features].fillna(0)      # limpieza de NaN (rolling al inicio)\ny = df[\"target\"]\n\n# chequeo final\nassert X.isna().sum().sum() == 0\nassert np.isfinite(X.values).all()\n\ntscv = TimeSeriesSplit(n_splits=5)\noof_enet = np.zeros(len(X))\noof_lgb  = np.zeros(len(X))\n\nfor fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx],   y.iloc[val_idx]\n    \n    # escalado\n    scaler = StandardScaler()\n    X_tr_s = scaler.fit_transform(X_tr)\n    X_val_s = scaler.transform(X_val)\n    \n    # ElasticNetCV\n    enet = ElasticNetCV(\n        l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n        alphas=np.logspace(-5, 0, 20),\n        cv=3,\n        max_iter=20000,\n        n_jobs=-1,\n        random_state=42\n    )\n    enet.fit(X_tr_s, y_tr)\n    oof_enet[val_idx] = enet.predict(X_val_s)\n    \n    # LightGBM\n    lgb_train = lgb.Dataset(X_tr, y_tr)\n    lgb_val   = lgb.Dataset(X_val, y_val, reference=lgb_train)\n    \n    lgb_model = lgb.train(\n        {\n            'objective': 'regression',\n            'metric': 'rmse',\n            'learning_rate': 0.05,\n            'num_leaves': 64,\n            'max_depth': 8,\n            'min_data_in_leaf': 20,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'verbosity': -1,\n            'seed': 42\n        },\n        lgb_train,\n        num_boost_round=2000,\n        valid_sets=[lgb_val],\n        callbacks=[lgb.early_stopping(80), lgb.log_evaluation(0)]\n    )\n    \n    oof_lgb[val_idx] = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)\n    \n    rmse_enet = mean_squared_error(y_val, oof_enet[val_idx], squared=False)\n    rmse_lgb  = mean_squared_error(y_val, oof_lgb[val_idx],  squared=False)\n    print(f\"Fold {fold+1} - ENet RMSE: {rmse_enet:.6f} | LGB RMSE: {rmse_lgb:.6f}\")\n\n# stacking con Ridge\nmeta_X = np.column_stack([oof_enet, oof_lgb])\nmeta_model = RidgeCV(alphas=np.logspace(-6, 6, 13))\nmeta_model.fit(meta_X, y)\n\nprint(f\"Ridge alpha: {meta_model.alpha_:.2e}\")\nprint(f\"Pesos -> ENet: {meta_model.coef_[0]:.4f} | LGB: {meta_model.coef_[1]:.4f}\")\nprint(f\"OOF RMSE stacked: {mean_squared_error(y, meta_model.predict(meta_X), squared=False):.6f}\")\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## 3. Entrenamiento Final + GARCH\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-12-04T16:20:40.586080Z\",\"iopub.execute_input\":\"2025-12-04T16:20:40.586337Z\",\"iopub.status.idle\":\"2025-12-04T16:21:00.019374Z\",\"shell.execute_reply.started\":\"2025-12-04T16:20:40.586318Z\",\"shell.execute_reply\":\"2025-12-04T16:21:00.018408Z\"}}\n# Entrenamiento final con todo el dataset (ya tenés X e y limpios)\n\nscaler_final = StandardScaler()\nX_scaled = scaler_final.fit_transform(X)\n\n# ElasticNet final con los mejores hiperparámetros encontrados en CV\nfinal_enet = ElasticNetCV(\n    l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n    alphas=np.logspace(-6, -1, 20),\n    cv=5,\n    max_iter=50000,\n    n_jobs=-1,\n    random_state=42\n)\nfinal_enet.fit(X_scaled, y)\nprint(f\"ElasticNet final -> alpha: {final_enet.alpha_:.2e}, l1_ratio: {final_enet.l1_ratio_:.2f}\")\n\n# LightGBM final\nlgb_params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'learning_rate': 0.03,\n    'num_leaves': 128,\n    'max_depth': 9,\n    'min_data_in_leaf': 30,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbosity': -1,\n    'seed': 42\n}\n\nfinal_lgb = lgb.train(\n    lgb_params,\n    lgb.Dataset(X, label=y),\n    num_boost_round=3000\n)\n\n# Predicciones en train para GARCH\nenet_pred = final_enet.predict(X_scaled)\nlgb_pred = final_lgb.predict(X)\nstack_pred = meta_model.predict(np.column_stack([enet_pred, lgb_pred]))\n\nresiduals = y - stack_pred\n\n# GARCH(1,1) con Student-t\ngarch = arch_model(residuals, vol='Garch', p=1, q=1, dist='StudentsT')\ngarch_fit = garch.fit(disp='off')\nprint(garch_fit.summary())\n\n# Guardar todo\njoblib.dump({\n    'scaler': scaler_final,\n    'enet': final_enet,\n    'lgb': final_lgb,\n    'meta': meta_model,\n    'garch': garch_fit,\n    'features': X.columns.tolist(),\n    'residuals_hist': residuals.values[-500:]  # para warm-up online\n}, 'final_model.pkl')\n\nprint(\"Todo guardado en final_model.pkl - listo para inference\")\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# ## 4. Inference Server (Kaggle Submission)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-12-04T16:21:00.020897Z\",\"iopub.execute_input\":\"2025-12-04T16:21:00.021205Z\",\"iopub.status.idle\":\"2025-12-04T16:21:00.048414Z\",\"shell.execute_reply.started\":\"2025-12-04T16:21:00.021183Z\",\"shell.execute_reply\":\"2025-12-04T16:21:00.047030Z\"}}\nimport kaggle_evaluation.default_inference_server as inf\nimport joblib\nimport numpy as np\nimport polars as pl\n\n# Cargar modelo\nmodel = joblib.load('final_model.pkl')\nscaler_final = model['scaler']\nfinal_enet   = model['enet']\nfinal_lgb    = model['lgb']\nmeta_model   = model['meta']\nfeatures     = model['features']   # <-- esto tiene que venir del entrenamiento\n\nstate = {\"last_allocation\": 0.0}\n\ndef add_features(df: pl.DataFrame) -> pl.DataFrame:\n    df = df.sort(\"date_id\")\n    # Solo columnas que existen en df y en features\n    safe_cols = [c for c in df.columns if c in features]\n    exprs = []\n    for col in safe_cols:\n        if col in [\"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"]:\n            continue\n        exprs.extend([\n            pl.col(col).rolling_mean(5).fill_null(0).alias(f\"{col}_m5\"),\n            pl.col(col).rolling_std(5).fill_null(0).alias(f\"{col}_s5\"),\n            pl.col(col).rolling_mean(21).fill_null(0).alias(f\"{col}_m21\"),\n            pl.col(col).rolling_std(21).fill_null(0).alias(f\"{col}_s21\"),\n            pl.col(col).shift(1).fill_null(0).alias(f\"{col}_lag1\"),\n            pl.col(col).shift(5).fill_null(0).alias(f\"{col}_lag5\"),\n        ])\n    df = df.with_columns(exprs).fill_null(0)\n    \n    # Asegurar que solo dejamos columnas que están en features\n    cols_to_keep = [c for c in features if c in df.columns]\n    return df.select(cols_to_keep)\n\n\ndef predict(test: pl.DataFrame) -> float:\n    global state\n    test = add_features(test)\n    # Aquí aseguramos que X_test solo tenga las columnas presentes y en el orden correcto\n    present_features = [f for f in features if f in test.columns]\n    X_test = test.select(present_features).to_pandas()\n    # Reindex para asegurarnos de que todas las features del modelo estén (si no, rellenar con 0)\n    X_test = X_test.reindex(columns=features, fill_value=0.0)\n\n    X_scaled = scaler_final.transform(X_test)\n    pred_enet = final_enet.predict(X_scaled)[0]\n    pred_lgb = final_lgb.predict(X_test)[0]\n    pred = meta_model.predict([[pred_enet, pred_lgb]])[0]\n\n    vol_cols = [c for c in features if any(x in c for x in [\"_s21\", \"V1\", \"V2\", \"V3\"]) and c in X_test.columns]\n    if vol_cols:\n        vol = X_test[vol_cols].abs().mean().mean()\n    else:\n        vol = 0.012\n    vol = max(float(vol), 0.006)\n\n    signal = pred / vol\n    allocation = np.clip(signal * 1.65, 0.0, 2.0)\n    smoothed = 0.92 * state[\"last_allocation\"] + 0.08 * allocation\n    state[\"last_allocation\"] = smoothed\n\n    return float(smoothed)\n\n\n# Server\nserver = inf.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    server.serve()\nelse:\n    server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"_uuid":"f86679bf-a1a1-487e-8581-5489c20b61f6","_cell_guid":"d5e09813-3833-4daa-bf8b-c03666b3848f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}