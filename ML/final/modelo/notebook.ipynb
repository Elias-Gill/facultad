{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Modelo de Posicionamiento en Mercado (0-2x)\n",
                "\n",
                "Este notebook contiene todo el proceso: carga de datos, creación del target, entrenamiento con validación temporal, stacking con meta-modelo y guardado. Al final se incluye la sección de predicción y cálculo de métricas.\n",
                "\n",
                "Objetivo: predecir la mejor posición diaria en el mercado (0 = cash, 1 = 100% mercado, 2 = 200% apalancado) usando retornos forward excess como señal."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Importaciones y configuración inicial"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import warnings\n",
                "import joblib\n",
                "import lightgbm as lgb\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import polars as pl\n",
                "from sklearn.linear_model import ElasticNetCV, RidgeCV\n",
                "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
                "from sklearn.model_selection import TimeSeriesSplit\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Carga y preparación de datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_path = \"./train.csv\"\n",
                "assert os.path.exists(train_path), \"No se encuentra train.csv\"\n",
                "\n",
                "train = pl.read_csv(train_path)\n",
                "train = train.rename({\"market_forward_excess_returns\": \"excess_return\"})\n",
                "train = train.with_columns(pl.all().cast(pl.Float64, strict=False))\n",
                "train = train.drop_nulls(subset=[\"excess_return\"])\n",
                "\n",
                "df = train.to_pandas().set_index(\"date_id\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Creación del target óptimo [0, 2]\n",
                "\n",
                "Transformamos el exceso de retorno futuro en una posición deseada:\n",
                "- Multiplicamos por 40 (factor de escala aproximado para llevar señales a rango útil).\n",
                "- Clip a [-1, 1] y luego convertimos a solo positivo [0, 2] (sin posiciones cortas).\n",
                "- 0 = 100% cash, 1 = 100% mercado, 2 = 200% apalancado."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Factor 40 es aproximado, se puede ajustar según volatilidad histórica\n",
                "df[\"target\"] = np.clip(df[\"excess_return\"] * 40, -1.0, 1.0)\n",
                "df[\"target\"] = df[\"target\"].clip(lower=0) * 2.0  # rango [0, 2]\n",
                "\n",
                "features = [c for c in df.columns if c not in [\"date_id\", \"excess_return\", \"forward_returns\", \"risk_free_rate\", \"target\"]]\n",
                "X = df[features].fillna(0)\n",
                "y = df[\"target\"].values\n",
                "\n",
                "print(f\"Usando {len(features)} features | Target ahora es posición [0-2]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Validación temporal y entrenamiento out-of-fold"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tscv = TimeSeriesSplit(n_splits=5)\n",
                "oof_enet = np.zeros(len(X))\n",
                "oof_lgb = np.zeros(len(X))\n",
                "\n",
                "for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
                "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
                "    y_tr, y_val = y[train_idx], y[val_idx]\n",
                "    \n",
                "    scaler = StandardScaler()\n",
                "    X_tr_s = scaler.fit_transform(X_tr)\n",
                "    X_val_s = scaler.transform(X_val)\n",
                "    \n",
                "    # ElasticNet\n",
                "    enet = ElasticNetCV(\n",
                    "        l1_ratio=[0.1, 0.5, 0.9, 1.0],\n",
                    "        alphas=np.logspace(-5, 1, 15),\n",
                    "        cv=3,\n",
                    "        max_iter=20000,\n",
                    "        n_jobs=-1,\n",
                    "        random_state=42,\n",
                    "    )\n",
                    "    enet.fit(X_tr_s, y_tr)\n",
                    "    oof_enet[val_idx] = np.clip(enet.predict(X_val_s), 0, 2)\n",
                    "    \n",
                    "    # LightGBM\n",
                    "    lgb_model = lgb.train(\n",
                        "        {\n",
                            "            \"objective\": \"regression\",\n",
                            "            \"metric\": \"rmse\",\n",
                            "            \"learning_rate\": 0.05,\n",
                            "            \"num_leaves\": 100,\n",
                            "            \"max_depth\": 10,\n",
                            "            \"min_data_in_leaf\": 30,\n",
                            "            \"feature_fraction\": 0.8,\n",
                            "            \"bagging_fraction\": 0.8,\n",
                            "            \"bagging_freq\": 5,\n",
                            "            \"verbosity\": -1,\n",
                            "            \"seed\": 42,\n",
                            "        },\n",
                            "        lgb.Dataset(X_tr, y_tr),\n",
                            "        num_boost_round=2500,\n",
                            "        valid_sets=[lgb.Dataset(X_val, y_val)],\n",
                            "        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],\n",
                            "    )\n",
                            "    oof_lgb[val_idx] = np.clip(lgb_model.predict(X_val), 0, 2)\n",
                            "    \n",
                            "    print(f\"Fold {fold+1} - ENet: {root_mean_squared_error(y_val, oof_enet[val_idx]):.4f} | LGB: {root_mean_squared_error(y_val, oof_lgb[val_idx]):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Meta-modelo (stacking)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "meta_X = np.column_stack([oof_enet, oof_lgb])\n",
                "meta_model = RidgeCV(alphas=np.logspace(-3, 3, 13))\n",
                "meta_model.fit(meta_X, y)\n",
                "\n",
                "print(f\"Meta weights -> ENet: {meta_model.coef_[0]:.3f}, LGB: {meta_model.coef_[1]:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Entrenamiento final con todos los datos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler_final = StandardScaler().fit(X)\n",
                "X_scaled = scaler_final.transform(X)\n",
                "\n",
                "final_enet = ElasticNetCV(\n",
                    "    l1_ratio=[0.1, 0.5, 0.9, 1.0],\n",
                    "    alphas=np.logspace(-5, 1, 20),\n",
                    "    cv=5,\n",
                    "    max_iter=50000,\n",
                    "    n_jobs=-1,\n",
                    "    random_state=42,\n",
                    ")\n",
                    "final_enet.fit(X_scaled, y)\n",
                    "\n",
                    "final_lgb = lgb.train(\n",
                        "    {\n",
                            "        \"objective\": \"regression\",\n",
                            "        \"learning_rate\": 0.03,\n",
                            "        \"num_leaves\": 128,\n",
                            "        \"max_depth\": 10,\n",
                            "        \"min_data_in_leaf\": 30,\n",
                            "        \"feature_fraction\": 0.7,\n",
                            "        \"bagging_fraction\": 0.7,\n",
                            "        \"bagging_freq\": 5,\n",
                            "        \"verbosity\": -1,\n",
                            "        \"seed\": 42,\n",
                            "    },\n",
                            "    lgb.Dataset(X, y),\n",
                            "    num_boost_round=5000,\n",
                            ")\n",
                            "\n",
                            "# Guardar modelo completo\n",
                            "joblib.dump(\n",
                                "    {\n",
                                    "        \"scaler\": scaler_final,\n",
                                    "        \"enet\": final_enet,\n",
                                    "        \"lgb\": final_lgb,\n",
                                    "        \"meta\": meta_model,\n",
                                    "        \"features\": features,\n",
                                    "    },\n",
                                    "    \"final_model.pkl\",\n",
                                    ")\n",
                                    "\n",
                                    "print(\"Modelo final guardado como final_model.pkl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Resumen de parámetros finales"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n=== Resumen de entrenamiento final ===\")\n",
                "print(f\"Features usadas: {len(features)}\")\n",
                "print(f\"ElasticNet final:\")\n",
                "print(f\" - Mejor alpha: {final_enet.alpha_:.6f}\")\n",
                "print(f\" - Mejor l1_ratio: {final_enet.l1_ratio_}\")\n",
                "print(f\" - Número de iteraciones: {final_enet.n_iter_}\")\n",
                "\n",
                "print(\"\\nLightGBM final:\")\n",
                "params_lgb = final_lgb.params\n",
                "print(f\" - learning_rate: {params_lgb.get('learning_rate', 'N/A')}\")\n",
                "print(f\" - num_leaves: {params_lgb.get('num_leaves', 'N/A')}\")\n",
                "print(f\" - max_depth: {params_lgb.get('max_depth', 'N/A')}\")\n",
                "print(f\" - min_data_in_leaf: {params_lgb.get('min_data_in_leaf', 'N/A')}\")\n",
                "print(f\" - Número de boosting rounds: {final_lgb.current_iteration()}\")\n",
                "\n",
                "print(\"\\nMeta modelo RidgeCV:\")\n",
                "print(f\" - Mejor alpha: {meta_model.alpha_}\")\n",
                "print(f\" - Coeficientes: ENet = {meta_model.coef_[0]:.4f}, LGB = {meta_model.coef_[1]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Predicción y métricas (sobre test y train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cargar modelo (si ya está entrenado)\n",
                "model = joblib.load(\"final_model.pkl\")\n",
                "scaler = model[\"scaler\"]\n",
                "enet = model[\"enet\"]\n",
                "lgb = model[\"lgb\"]\n",
                "meta = model[\"meta\"]\n",
                "features = model[\"features\"]\n",
                "\n",
                "def prepare_X(df: pl.DataFrame) -> np.ndarray:\n",
                "    missing = [f for f in features if f not in df.columns]\n",
                "    if missing:\n",
                "        df = df.with_columns([pl.lit(0.0).alias(f) for f in missing])\n",
                "    X = df.select(features).fill_null(0).to_numpy().astype(np.float64)\n",
                "    if np.isnan(X).any():\n",
                "        X = np.nan_to_num(X, nan=0.0)\n",
                "    return X\n",
                "\n",
                "# Cargar datos una sola vez\n",
                "test_df = pl.read_csv(\"test.csv\")\n",
                "train_df = pl.read_csv(\"train.csv\")\n",
                "\n",
                "# Predicción en test\n",
                "X_test = prepare_X(test_df)\n",
                "X_test_s = scaler.transform(X_test)\n",
                "pred_enet = enet.predict(X_test_s)\n",
                "pred_lgb = lgb.predict(X_test)\n",
                "position = np.clip(meta.predict(np.column_stack([pred_enet, pred_lgb])), 0, 2)\n",
                "\n",
                "submission = pd.DataFrame({\n",
                    "    \"date_id\": test_df[\"date_id\"].to_list(),\n",
                    "    \"position\": position.flatten()\n",
                    "})\n",
                    "if \"is_scored\" in test_df.columns:\n",
                    "    submission = submission[test_df[\"is_scored\"].to_numpy().astype(bool)].reset_index(drop=True)\n",
                    "\n",
                    "submission.to_csv(\"submission.csv\", index=False)\n",
                    "print(f\"SUBMISSION → {len(submission)} días puntuables → submission.csv guardado\")\n",
                    "\n",
                    "# Métricas en train\n",
                    "X_train = prepare_X(train_df)\n",
                    "X_train_s = scaler.transform(X_train)\n",
                    "pred_enet_train = enet.predict(X_train_s)\n",
                    "pred_lgb_train = lgb.predict(X_train)\n",
                    "position_train = np.clip(meta.predict(np.column_stack([pred_enet_train, pred_lgb_train])), 0, 2)\n",
                    "\n",
                    "excess = train_df[\"market_forward_excess_returns\"].to_numpy()\n",
                    "strategy_ret = position_train.flatten() * excess\n",
                    "ann_ret = strategy_ret.mean() * 252\n",
                    "ann_vol = strategy_ret.std() * np.sqrt(252)\n",
                    "sharpe = ann_ret / ann_vol if ann_vol > 0 else 0\n",
                    "market_vol = excess.std() * np.sqrt(252)\n",
                    "vol_ratio = ann_vol / (1.2 * market_vol)\n",
                    "penalty = max(vol_ratio - 1, 0) ** 2\n",
                    "adj_sharpe = sharpe * (1 - penalty)\n",
                    "mae = np.mean(np.abs(strategy_ret - excess))\n",
                    "rmse = np.sqrt(np.mean((strategy_ret - excess)**2))\n",
                    "\n",
                    "print(\"\\n\" + \"=\"*60)\n",
                    "print(\" RESULTADOS FINALES - TU ESTRATEGIA (STACKING)\")\n",
                    "print(\"=\"*60)\n",
                    "print(f\"Retorno anualizado : {ann_ret*100:6.2f}%\")\n",
                    "print(f\"Volatilidad anualizada : {ann_vol*100:6.2f}%\")\n",
                    "print(f\"Sharpe Ratio : {sharpe:.4f}\")\n",
                    "print(f\"Sharpe Ajustado : {adj_sharpe:.4f}\")\n",
                    "print(f\"MAE vs Benchmark : {mae:.6f}\")\n",
                    "print(f\"RMSE vs Benchmark : {rmse:.6f}\")\n",
                    "print(f\"Vol vs límite 120% : {vol_ratio:.3f}x → penalización {penalty:.1%}\")\n",
                    "print(f\"Días evaluados : {len(excess)}\")\n",
                    "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
